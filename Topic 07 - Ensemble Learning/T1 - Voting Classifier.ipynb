{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Majority vote classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setting up\n",
    "- Iris data\n",
    "- Binary classification\n",
    "- 2 features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Iris data\n",
    "dataObj = load_iris()\n",
    "\n",
    "# X data\n",
    "X = dataObj.data[50:, [1, 2]]\n",
    "\n",
    "# y data. We will use only two classes (for ROC AUC calculation).\n",
    "y = dataObj.target[50:]\n",
    "\n",
    "# Set the class label to 0 and 1\n",
    "y = np.where(y == 2, 1, 0)\n",
    "print(np.unique(y))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1, stratify=y)"
   ]
  },
  {
   "source": [
    "## Training performance of individual classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "#Logistic regression\n",
    "clf1 = LogisticRegression(penalty='l2', C=0.001, random_state=0)\n",
    "pipe_lr = Pipeline([['sc', StandardScaler()], ['clf', clf1]])\n",
    "\n",
    "# Decision tree\n",
    "clf2 = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=0)\n",
    "pipe_dt = Pipeline([['sc', StandardScaler()], ['clf', clf2]])\n",
    "\n",
    "# KNN\n",
    "clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')\n",
    "pipe_knn = Pipeline([['sc', StandardScaler()], ['clf', clf3]])\n",
    "\n",
    "pipes = [pipe_lr, pipe_dt, pipe_knn]\n",
    "names = ['Logistic Regression', 'Decision Tree', 'K-Nearest Neighbor']\n",
    "\n",
    "# Empty dataframe\n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "# 10-fold stratified cross validation\n",
    "for pipe, name in zip(pipes, names):\n",
    "\n",
    "    # Accuracy\n",
    "    ACC = cross_val_score(estimator=pipe, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "    # Precision (need to use make_scorer because I need to pass zero_division=0 argument)\n",
    "    scorer_precision = make_scorer(precision_score, zero_division=0, pos_label=1)\n",
    "    PRE = cross_val_score(estimator=pipe, X=X_train, y=y_train, cv=10, scoring=scorer_precision)\n",
    "\n",
    "    # Recall\n",
    "    REC = cross_val_score(estimator=pipe, X=X_train, y=y_train, cv=10, scoring='recall')\n",
    "\n",
    "    # ROC AUC\n",
    "    ROCAUC = cross_val_score(estimator=pipe, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n",
    "\n",
    "    data = {'clf': name,\n",
    "            'ACC Mean': f\"{ACC.mean():6.3f} (+/- {ACC.std():6.3f})\",\n",
    "            'PRE Mean': f\"{PRE.mean():6.3f} (+/- {PRE.std():6.3f})\",\n",
    "            'REC Mean': f\"{REC.mean():6.3f} (+/- {REC.std():6.3f})\",\n",
    "            'ROC-AUC Mean': f\"{ROCAUC.mean():6.3f} +/- {ROCAUC.std():6.3f}\"}\n",
    "    df1 = df1.append(data, ignore_index=True)\n",
    "\n",
    "df1 = df1.set_index([\"clf\"])\n",
    "display(df1)"
   ]
  },
  {
   "source": [
    "## Training performance of majority vote classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Ensemble estimator\n",
    "estimators = [\n",
    "    ('pipe_lr', pipe_lr),\n",
    "    ('pipe_dt', pipe_dt),\n",
    "    ('pipe_knn', pipe_knn)]\n",
    "    \n",
    "eclf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Accuracy\n",
    "ACC = cross_val_score(estimator=eclf, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "\n",
    "# Precision\n",
    "scorer_precision = make_scorer(precision_score, zero_division=0, pos_label=1)\n",
    "PRE = cross_val_score(estimator=eclf, X=X_train, y=y_train, cv=10, scoring=scorer_precision)\n",
    "\n",
    "# Recall\n",
    "REC = cross_val_score(estimator=eclf, X=X_train, y=y_train, cv=10, scoring='recall')\n",
    "\n",
    "# ROC AUC\n",
    "ROCAUC = cross_val_score(estimator=eclf, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "data = {'clf': 'Ensemble',\n",
    "        'ACC Mean': f\"{ACC.mean():6.3f} (+/- {ACC.std():6.3f})\",\n",
    "        'PRE Mean': f\"{PRE.mean():6.3f} (+/- {PRE.std():6.3f})\",\n",
    "        'REC Mean': f\"{REC.mean():6.3f} (+/- {REC.std():6.3f})\",\n",
    "        'ROC-AUC Mean': f\"{ROCAUC.mean():6.3f} +/- {ROCAUC.std():6.3f}\"}\n",
    "df2 = df2.append(data, ignore_index=True)\n",
    "df2 = df2.set_index([\"clf\"])\n",
    "\n",
    "df = pd.concat((df1,df2))\n",
    "display(df)"
   ]
  },
  {
   "source": [
    "## Making prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "eclf.fit(X_train, y_train)\n",
    "print(eclf.predict(X_test))"
   ]
  },
  {
   "source": [
    "## Testing performances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "pipes = [pipe_lr, pipe_dt, pipe_knn, eclf]\n",
    "names = ['Logistic Regression', 'Decision Tree', 'K-Nearest Neighbor', 'Ensemble']\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for pipe, name in zip(pipes, names):\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = pipe.predict_proba(X_test)\n",
    "\n",
    "    ACC = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    PRE = precision_score(y_true=y_test, y_pred=y_pred)\n",
    "    REC = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "    F1 = f1_score(y_true=y_test, y_pred=y_pred)\n",
    "    ROCAUC = roc_auc_score(y_true=y_test, y_score=y_proba[:,1])\n",
    "\n",
    "    data = {'name': name, 'ACC': ACC, 'PRE': PRE, 'REC': REC, 'F1': F1, 'ROCAUC': ROCAUC}\n",
    "\n",
    "    df = df.append(data, ignore_index=True)\n",
    "\n",
    "df = df.set_index(['name'])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "ax = plt.gca()\n",
    "for pipe in pipes:\n",
    "    plot_roc_curve(pipe, X_test,y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "all_clf = [pipe_lr, pipe_dt, pipe_knn, eclf]\n",
    "clf_labels = [\"LR\", \"DT\", \"KNN\", \"EN\"]\n",
    "\n",
    "x_min = X_train[:, 0].min() - 1\n",
    "x_max = X_train[:, 0].max() + 1\n",
    "y_min = X_train[:, 1].min() - 1\n",
    "y_max = X_train[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(nrows=2, ncols=2, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1]), all_clf, clf_labels):\n",
    "\n",
    "    clf.fit(X_train, y_train)    \n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)\n",
    "    \n",
    "    axarr[idx[0], idx[1]].scatter(X_test[y_test==0, 0], \n",
    "                                  X_test[y_test==0, 1], \n",
    "                                  c='blue', \n",
    "                                  marker='^',\n",
    "                                  s=50)\n",
    "    \n",
    "    axarr[idx[0], idx[1]].scatter(X_test[y_test==1, 0], \n",
    "                                  X_test[y_test==1, 1], \n",
    "                                  c='red', \n",
    "                                  marker='o',\n",
    "                                  s=50)\n",
    "    \n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "    axarr[idx[0], idx[1]].set_xlabel(\"Petal length\")\n",
    "    axarr[idx[0], idx[1]].set_ylabel(\"Sepal width\")\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Tuning parameter using grid search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameter names\n",
    "for k, v in eclf.get_params().items():\n",
    "    print(f\"{k:35.35s}: {str(v):35.35}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameters\n",
    "params = {'pipe_dt__clf__max_depth': [1, 2], 'pipe_lr__clf__C': [0.001, 0.01, 0.1]}\n",
    "\n",
    "# Grid search using ROC AUC as a score\n",
    "gs = GridSearchCV(estimator=eclf, param_grid=params, scoring='roc_auc', cv=10, n_jobs=-1)\n",
    "\n",
    "# Searching\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# Best estimator\n",
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gs.cv_results_)\n",
    "display(df)"
   ]
  }
 ]
}