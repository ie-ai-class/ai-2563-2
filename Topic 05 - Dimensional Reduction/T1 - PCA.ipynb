{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Principal component analysis\n",
    "Unsupervised dimensionality reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Method 1: Using NumPy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Load data, split data, standardization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_wine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load data\n",
    "dataObj = load_wine()\n",
    "X = dataObj.data\n",
    "y = dataObj.target\n",
    "\n",
    "# Create DataFrame with features\n",
    "df = pd.DataFrame(X)\n",
    "df.columns = dataObj.feature_names\n",
    "\n",
    "# Add class column\n",
    "df.insert(loc=0, column=\"Class\", value=y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "source": [
    "### Standardize the original dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "dfOri = pd.DataFrame(X_train_std)\n",
    "display(dfOri)"
   ]
  },
  {
   "source": [
    "### Construct covarince matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(X_train_std, rowvar=False)\n",
    "dfCov = pd.DataFrame(cov_mat)\n",
    "display(dfCov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(dfOri.corr(), cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax, vmin=-1, vmax=1)"
   ]
  },
  {
   "source": [
    "### Eigendecomposition of the covariance matrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort eigenvalues\n",
    "idx = np.argsort(eigen_vals)\n",
    "idx = idx[::-1] #Sort from max to min\n",
    "eigen_vals = eigen_vals[idx]\n",
    "eigen_vecs = eigen_vecs[:,idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eigenvectors\n",
    "display(pd.DataFrame(eigen_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eigenvalues\n",
    "display(pd.DataFrame(eigen_vals.reshape(1,-1)))"
   ]
  },
  {
   "source": [
    "### Total and explained variance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eigen_vals)\n",
    "var_explained = eigen_vals/tot\n",
    "cum_var_explained = np.cumsum(var_explained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1, 14), var_explained, alpha=0.5, align='center', label='individual explained variance')\n",
    "plt.step(range(1, 14), cum_var_explained, where='mid', label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/pca1.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Transformation matrix, W"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = eigen_vecs[:,[0,1]]\n",
    "display(pd.DataFrame(w))"
   ]
  },
  {
   "source": [
    "### Transforming traning data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = X_train_std.dot(w)\n",
    "display(pd.DataFrame(X_train_pca))"
   ]
  },
  {
   "source": [
    "### Visualizing training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PlotFunction3 import plot_reduced_dim\n",
    "plot_reduced_dim(X_train_pca, y_train, \"PCA\")"
   ]
  },
  {
   "source": [
    "### Note\n",
    "- It is possible to have Matrix W with its signs flipped. \n",
    "- If $v$ is an eigenvector of a matrix $\\Sigma$, we have $\\Sigma v = \\lambda v,$ where $\\lambda$ is our eigenvalue.\n",
    "- Then $-v$ is also an eigenvector that has the same eigenvalue, since $\\Sigma(-v) = -\\Sigma v = -\\lambda v = \\lambda(-v).$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using eigenvectors with different signs\n",
    "w2 = np.copy(w)\n",
    "w2[:,0] = -w2[:,0]\n",
    "X_train_pca = X_train_std.dot(w2)\n",
    "plot_reduced_dim(X_train_pca, y_train, \"PCA\")\n",
    "\n",
    "w2 = np.copy(w)\n",
    "w2[:,1] = -w2[:,1]\n",
    "X_train_pca = X_train_std.dot(w2)\n",
    "plot_reduced_dim(X_train_pca, y_train, \"PCA\")\n",
    "\n",
    "w2 = np.copy(w)\n",
    "w2 = -w2\n",
    "X_train_pca = X_train_std.dot(w2)\n",
    "plot_reduced_dim(X_train_pca, y_train, \"PCA\")\n"
   ]
  },
  {
   "source": [
    "## Method 2: Using SKL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(1, 14), pca.explained_variance_ratio_, alpha=0.5, align='center')\n",
    "plt.step(range(1, 14), np.cumsum(pca.explained_variance_ratio_), where='mid')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reduced_dim(X_train_pca, y_train, \"PCA\")"
   ]
  },
  {
   "source": [
    "## Training with logistic regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PlotFunction2 import plot_decision_surface2\n",
    "\n",
    "plot_decision_surface2(X_train_pca, X_test_pca, y_train, y_test, lr)"
   ]
  }
 ]
}